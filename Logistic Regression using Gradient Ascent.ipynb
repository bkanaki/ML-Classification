{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the required libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhargav/anaconda/envs/gl-env/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2885: FutureWarning: \n",
      "mpl_style had been deprecated and will be removed in a future version.\n",
      "Use `matplotlib.pyplot.style.use` instead.\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.mpl_style', 'default') \n",
    "pd.set_option('display.width', 5000) \n",
    "pd.set_option('display.max_columns', 60) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Stop Pacifier Sucking without tears with Thumb...\n",
       "1      Nature's Lullabies Second Year Sticker Calendar\n",
       "2      Nature's Lullabies Second Year Sticker Calendar\n",
       "3                          Lamaze Peekaboo, I Love You\n",
       "4    SoftPlay Peek-A-Boo Where's Elmo A Children's ...\n",
       "5                            Our Baby Girl Memory Book\n",
       "6    Hunnt&reg; Falling Flowers and Birds Kids Nurs...\n",
       "7    Blessed By Pope Benedict XVI Divine Mercy Full...\n",
       "8    Cloth Diaper Pins Stainless Steel Traditional ...\n",
       "9    Cloth Diaper Pins Stainless Steel Traditional ...\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "products = pd.read_csv('amazon_baby_subset.csv')\n",
    "products.head(10)['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    26579\n",
      "-1    26493\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1171b1350>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEECAYAAADOJIhPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKl6jDVgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10417bb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_count = products['sentiment'].value_counts()\n",
    "print(sentiment_count)\n",
    "sentiment_count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load important words and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "important_words = pd.read_json('important_words.json')\n",
    "# Convert to list to get the Bag-of-Words count\n",
    "important_words = list(important_words.astype(str).values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products = products.fillna({'review':''})  # fill in N/A's in the review column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation from all the reiews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None, string.punctuation)\n",
    "\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Bag-of-Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is slow, could explore different ways to do this. Probably this is the best.\n",
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question.** How many reviews contain the word perfect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews that contain word perfect:  2955\n"
     ]
    }
   ],
   "source": [
    "products['contains_perfect'] = products['perfect'].apply(lambda i : 1 if i >= 1 else 0)\n",
    "perfect_count = products['contains_perfect'].sum()\n",
    "print 'Number of reviews that contain word perfect: ', perfect_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data frame to multi-dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(dataframe, features, label):\n",
    "    dataframe['constant'] = 1\n",
    "    features = ['constant'] + features\n",
    "    features_frame = dataframe[features]\n",
    "    feature_matrix = features_frame.as_matrix()\n",
    "    label_sarray = dataframe[label]\n",
    "    label_array = label_sarray.as_matrix()\n",
    "    return(feature_matrix, label_array)\n",
    "\n",
    "(feature_matrix, sentiment) = get_numpy_data(products, important_words, 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** How many features are there in the feature_matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features are:  194\n"
     ]
    }
   ],
   "source": [
    "print 'Number of features are: ', feature_matrix.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating conditional probability with link function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "produces probablistic estimate for P(y_i = +1 | x_i, w).\n",
    "estimate ranges between 0 and 1.\n",
    "'''\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    score = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    predictions = np.reciprocal(1 + np.exp(-score))\n",
    "    \n",
    "    # return predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following outputs must match \n",
      "------------------------------------------------\n",
      "correct_predictions           = [ 0.98201379  0.26894142]\n",
      "output of predict_probability = [ 0.98201379  0.26894142]\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint\n",
    "dummy_feature_matrix = np.array([[1.,2.,3.], [1.,-1.,-1]])\n",
    "dummy_coefficients = np.array([1., 3., -1.])\n",
    "\n",
    "correct_scores      = np.array( [ 1.*1. + 2.*3. + 3.*(-1.),          1.*1. + (-1.)*3. + (-1.)*(-1.) ] )\n",
    "correct_predictions = np.array( [ 1./(1+np.exp(-correct_scores[0])), 1./(1+np.exp(-correct_scores[1])) ] )\n",
    "\n",
    "print 'The following outputs must match '\n",
    "print '------------------------------------------------'\n",
    "print 'correct_predictions           =', correct_predictions\n",
    "print 'output of predict_probability =', predict_probability(dummy_feature_matrix, dummy_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute derivative of log likelihood with respect to a single coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):     \n",
    "    # Compute the dot product of errors and feature. feature is a vector and not a matrix.\n",
    "    derivative = np.dot(feature, errors)\n",
    "        # Return the derivative\n",
    "    return derivative\n",
    "\n",
    "def compute_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores)))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following outputs must match \n",
      "------------------------------------------------\n",
      "correct_log_likelihood           = -5.33141161544\n",
      "output of compute_log_likelihood = -5.33141161544\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint\n",
    "dummy_feature_matrix = np.array([[1.,2.,3.], [1.,-1.,-1]])\n",
    "dummy_coefficients = np.array([1., 3., -1.])\n",
    "dummy_sentiment = np.array([-1, 1])\n",
    "\n",
    "correct_indicators  = np.array( [ -1==+1,                                       1==+1 ] )\n",
    "correct_scores      = np.array( [ 1.*1. + 2.*3. + 3.*(-1.),                     1.*1. + (-1.)*3. + (-1.)*(-1.) ] )\n",
    "correct_first_term  = np.array( [ (correct_indicators[0]-1)*correct_scores[0],  (correct_indicators[1]-1)*correct_scores[1] ] )\n",
    "correct_second_term = np.array( [ np.log(1. + np.exp(-correct_scores[0])),      np.log(1. + np.exp(-correct_scores[1])) ] )\n",
    "\n",
    "correct_ll          =      sum( [ correct_first_term[0]-correct_second_term[0], correct_first_term[1]-correct_second_term[1] ] ) \n",
    "\n",
    "print 'The following outputs must match '\n",
    "print '------------------------------------------------'\n",
    "print 'correct_log_likelihood           =', correct_ll\n",
    "print 'output of compute_log_likelihood =', compute_log_likelihood(dummy_feature_matrix, dummy_sentiment, dummy_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in xrange(max_iter):\n",
    "        # Predict P(y_i = +1|x_1,w) using your predict_probability() function\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "\n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "\n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "\n",
    "        for j in xrange(len(coefficients)): # loop over each coefficient\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j]\n",
    "            # compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            derivative = feature_derivative(errors, feature_matrix[:,j])\n",
    "\n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            coefficients[j] += step_size*derivative\n",
    "\n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, sentiment, coefficients)\n",
    "            print 'iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp)\n",
    "    print 'Training Complete using Logitic Regression'\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -36780.91768478\n",
      "iteration   1: log likelihood of observed labels = -36775.13434712\n",
      "iteration   2: log likelihood of observed labels = -36769.35713564\n",
      "iteration   3: log likelihood of observed labels = -36763.58603240\n",
      "iteration   4: log likelihood of observed labels = -36757.82101962\n",
      "iteration   5: log likelihood of observed labels = -36752.06207964\n",
      "iteration   6: log likelihood of observed labels = -36746.30919497\n",
      "iteration   7: log likelihood of observed labels = -36740.56234821\n",
      "iteration   8: log likelihood of observed labels = -36734.82152213\n",
      "iteration   9: log likelihood of observed labels = -36729.08669961\n",
      "iteration  10: log likelihood of observed labels = -36723.35786366\n",
      "iteration  11: log likelihood of observed labels = -36717.63499744\n",
      "iteration  12: log likelihood of observed labels = -36711.91808422\n",
      "iteration  13: log likelihood of observed labels = -36706.20710739\n",
      "iteration  14: log likelihood of observed labels = -36700.50205049\n",
      "iteration  15: log likelihood of observed labels = -36694.80289716\n",
      "iteration  20: log likelihood of observed labels = -36666.39512033\n",
      "iteration  30: log likelihood of observed labels = -36610.01327118\n",
      "iteration  40: log likelihood of observed labels = -36554.19728365\n",
      "iteration  50: log likelihood of observed labels = -36498.93316099\n",
      "iteration  60: log likelihood of observed labels = -36444.20783914\n",
      "iteration  70: log likelihood of observed labels = -36390.00909449\n",
      "iteration  80: log likelihood of observed labels = -36336.32546144\n",
      "iteration  90: log likelihood of observed labels = -36283.14615871\n",
      "iteration 100: log likelihood of observed labels = -36230.46102347\n",
      "iteration 200: log likelihood of observed labels = -35728.89418769\n",
      "iteration 300: log likelihood of observed labels = -35268.51212683\n",
      "Training Complete using Logitic Regression\n"
     ]
    }
   ],
   "source": [
    "coefficients = logistic_regression(feature_matrix, sentiment, initial_coefficients=np.zeros(194),\n",
    "                                   step_size=1e-7, max_iter=301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "score = np.dot(feature_matrix, coefficients)\n",
    "yHat = np.less_equal(0, score).astype(int)\n",
    "yHat[(np.where(yHat == 0))] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz question:** How many reviews are predicted to have positive sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive sentiment prediction: 25126\n"
     ]
    }
   ],
   "source": [
    "print 'Number of positive sentiment prediction:' , np.count_nonzero(yHat == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz question:** What is the accuracy of the model on predictions made above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "# Reviews correctly classified   = 39903\n",
      "# Reviews incorrectly classified = 13169\n",
      "# Reviews total                  = 53072\n",
      "-----------------------------------------------------\n",
      "Accuracy = 0.75\n"
     ]
    }
   ],
   "source": [
    "num_datapoints = len(yHat)\n",
    "correct_pred = np.sum(yHat == sentiment)\n",
    "num_mistakes = num_datapoints - correct_pred\n",
    "accuracy = float(correct_pred)/num_datapoints\n",
    "print \"-----------------------------------------------------\"\n",
    "print '# Reviews correctly classified   =', len(products) - num_mistakes\n",
    "print '# Reviews incorrectly classified =', num_mistakes\n",
    "print '# Reviews total                  =', len(products)\n",
    "print \"-----------------------------------------------------\"\n",
    "print 'Accuracy = %.2f' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which words contribute most to positive & negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coefficients = list(coefficients[1:]) # exclude intercept\n",
    "word_coefficient_tuples = [(word, coefficient) for word, coefficient in zip(important_words, coefficients)]\n",
    "word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz question:** Which word is not present in the top 10 \"most positive\" words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.066546084170457695),\n",
       " ('love', 0.065890762922123258),\n",
       " ('easy', 0.06479458680257838),\n",
       " ('little', 0.045435626308421365),\n",
       " ('loves', 0.044976401394906038),\n",
       " ('well', 0.030135001092107077),\n",
       " ('perfect', 0.029739937104968462),\n",
       " ('old', 0.020077541034775378),\n",
       " ('nice', 0.018408707995268992),\n",
       " ('daughter', 0.017703199905701694)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_tuples[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz question:** Which word is not present in the top 10 \"most negative\" words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('monitor', -0.024482100545891717),\n",
       " ('return', -0.026592778462247283),\n",
       " ('back', -0.027742697230661334),\n",
       " ('get', -0.028711552980192574),\n",
       " ('disappointed', -0.028978976142317068),\n",
       " ('even', -0.030051249236035804),\n",
       " ('work', -0.03306951529475273),\n",
       " ('money', -0.038982037286487116),\n",
       " ('product', -0.041511033392108904),\n",
       " ('would', -0.053860148445203142)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_tuples[-10:]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
